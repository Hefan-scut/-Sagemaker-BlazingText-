{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "from random import shuffle\n",
    "import jieba\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'blazingtext/Sentiment_classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将金融情感词典导入jieba库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jieba.txt', 'r',encoding='gbk') as f:\n",
    "    jieba_list = f.read().splitlines()\n",
    "jieba.load_userdict(jieba_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理通联数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据读取\n",
    "# data = pd.read_csv('news_part.csv')\n",
    "# data.drop_duplicates(subset=['news_title'],keep='first',inplace=True)\n",
    "# data = data.applymap((lambda x:\"\".join(x.split()) if type(x) is str else x))\n",
    "# data['lable'] =['__label__%i' % i for i in data['SENTIMENT']]\n",
    "# data['aws_format'] = data['lable'] + '  ' + data['news_title']\n",
    "\n",
    "# #取各类情感数据各5000条\n",
    "# positive = data[data['SENTIMENT']==1].head(5000)\n",
    "# negative = data[data['SENTIMENT']==-1].head(5000)\n",
    "# neutral = data[data['SENTIMENT'] == 0].head(5000)\n",
    "\n",
    "# #整理并打乱数据\n",
    "# result = positive.append(negative)\n",
    "# result = result.append(neutral)\n",
    "# result = result.sample(frac=1)\n",
    "\n",
    "# #将数据处理成AWS需要的format，保存到data_new中\n",
    "# result['aws_format'].to_csv('data.txt',index=False,header=None,encoding='utf_8_sig')\n",
    "# lines = open('data.txt').readlines() #打开文件，读入每一行\n",
    "# file = open('data_new.txt','w') #打开你要写得文件pp2.txt\n",
    "# for s in lines:\n",
    "#     file.write(s.replace('\\\"','')) # replace是替换，write是写入\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用jieba将文本数据进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "file  = 'data_new.txt'\n",
    "with open(file,encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "labels = []\n",
    "for line in lines:\n",
    "    label = []\n",
    "    line = line.split('  ')\n",
    "    label.append(line[0])\n",
    "    line[1] = re.sub(r\"[\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\", line[1])\n",
    "    label.extend(jieba.cut(line[1],cut_all=False))\n",
    "    labels.append(label)\n",
    "    \n",
    "shuffle(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据分割成训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_data = labels[0:int(len(labels)*0.8)]\n",
    "t_validation_data = labels[int(len(labels)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "t_train_file = 'tt.train'\n",
    "t_validation_file = 'tt.validation'\n",
    "\n",
    "with open(t_train_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_train_data)\n",
    "    \n",
    "with open(t_validation_file, 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(t_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 194 ms, sys: 25.6 ms, total: 219 ms\n",
      "Wall time: 761 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t_train_channel = prefix + '/train'\n",
    "t_validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='tt.train', bucket=bucket, key_prefix=t_train_channel)\n",
    "sess.upload_data(path='tt.validation', bucket=bucket, key_prefix=t_validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, t_train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, t_validation_channel)\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 501404015308.dkr.ecr.ap-northeast-1.amazonaws.com/blazingtext:latest (ap-northeast-1)\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数配置 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "t_bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "t_bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            buckets=3396748,\n",
    "                            epochs=9,\n",
    "                            min_count=5,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=84,\n",
    "                            early_stopping=False,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "t_train_data = sagemaker.inputs.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_validation_data = sagemaker.inputs.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "t_data_channels = {'train': t_train_data, 'validation': t_validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-07 02:28:15 Starting - Starting the training job...\n",
      "2020-11-07 02:28:17 Starting - Launching requested ML instances......\n",
      "2020-11-07 02:29:32 Starting - Preparing the instances for training......\n",
      "2020-11-07 02:30:33 Downloading - Downloading input data\n",
      "2020-11-07 02:30:33 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 WARNING 140164474562368] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 WARNING 140164474562368] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 INFO 140164474562368] nvidia-smi took: 0.0251748561859 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 INFO 140164474562368] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 INFO 140164474562368] Processing /opt/ml/input/data/train/tt.train . File size: 13 MB\u001b[0m\n",
      "\u001b[34m[11/07/2020 02:30:49 INFO 140164474562368] Processing /opt/ml/input/data/validation/tt.validation . File size: 3 MB\u001b[0m\n",
      "\u001b[34mRead 2M words\u001b[0m\n",
      "\u001b[34mNumber of words:  24798\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0444  Progress: 11.14%  Million Words/sec: 21.78 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0389  Progress: 22.28%  Million Words/sec: 21.91 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0333  Progress: 33.39%  Million Words/sec: 21.91 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0277  Progress: 44.65%  Million Words/sec: 21.98 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0222  Progress: 55.61%  Million Words/sec: 21.91 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0166  Progress: 66.78%  Million Words/sec: 21.93 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0110  Progress: 78.02%  Million Words/sec: 21.98 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0054  Progress: 89.26%  Million Words/sec: 22.01 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.00%  Million Words/sec: 21.93 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 21.92 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 21.92\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 0.90\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9965\u001b[0m\n",
      "\u001b[34mNumber of train examples: 167993\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.8858\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 41998\u001b[0m\n",
      "\n",
      "2020-11-07 02:31:04 Uploading - Uploading generated training model\n",
      "2020-11-07 02:33:12 Completed - Training job completed\n",
      "Training seconds: 171\n",
      "Billable seconds: 171\n"
     ]
    }
   ],
   "source": [
    "t_bt_model.fit(inputs=t_data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "t_text_classifier = t_bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment_classification(sentences):\n",
    "    sentences = sentences\n",
    "    tokenized_sentences = [' '.join(jieba.cut(sentences,cut_all=False))]\n",
    "    payload = {\"instances\" : tokenized_sentences}\n",
    "    t_response = t_text_classifier.predict(json.dumps(payload))\n",
    "    t_predictions = json.loads(t_response)\n",
    "    predictions = re.sub(r\"[\\s+\\!\\/_$%^*()?;；:-【】+\\\"\\']+|[+——一！，;:：。？、~@#￥%……&*（）]+\", \"\",json.dumps(t_predictions, indent=0)).split(',')\n",
    "    result = {}\n",
    "    result['Sentence'] = sentences\n",
    "    result['Probility'] = float(predictions[0])\n",
    "    result['Category'] = int(predictions[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测集样例观看 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019年湘江金融发展峰会银行科技论坛落幕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>易纲:金融风险整体收敛货币政策工具手段充足</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>电价形成机制发生“质变”下半年电力领域混改或成重头戏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>三盛赶在五一给冯劲义送了一份礼物：三盛总裁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>是谁夺走了我们的注意力？BBC：警惕“数码黑帮”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  news_titles\n",
       "0       2019年湘江金融发展峰会银行科技论坛落幕\n",
       "1       易纲:金融风险整体收敛货币政策工具手段充足\n",
       "2  电价形成机制发生“质变”下半年电力领域混改或成重头戏\n",
       "3       三盛赶在五一给冯劲义送了一份礼物：三盛总裁\n",
       "4    是谁夺走了我们的注意力？BBC：警惕“数码黑帮”"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('news_prediction.csv')\n",
    "news.head()    #查看news_title样例内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "康美300亿造假退市还是罚60万？\n",
      "0.9991288781166077\n",
      "-1\n",
      "戴姆勒计划裁员至少1万人，占全球员工的3.3%\n",
      "0.9934340715408325\n",
      "-1\n",
      "第一上海(00227.HK)2018年度纯利减少46%至3340.3万港元\n",
      "0.9937437176704407\n",
      "-1\n",
      "特宝生物研发味淡:曾因业绩差终止辅导却想上科创板\n",
      "0.9998844861984253\n",
      "-1\n",
      "德邦股份一季度净利亏损4905万元现金流量净额为-2.3亿元\n",
      "0.9994333982467651\n",
      "-1\n",
      "经营现金流锐减3.1亿，华强方特与预收款不得不说的事\n",
      "0.9636446237564087\n",
      "-1\n",
      "钴曾被疯抢如今价格腰斩连累寒锐钴业市值蒸发275亿\n",
      "0.9908749461174011\n",
      "-1\n",
      "电解铜合同引纠纷海航期货计提7千多万坏账准备\n",
      "0.9732795357704163\n",
      "-1\n",
      "网易有道递交招股书：丁磊持股30%上半年净亏1.68亿元\n",
      "0.997651219367981\n",
      "-1\n",
      "康健国际医疗(03886)将继续与证监会沟通寻求尽快复牌\n",
      "0.9511836767196655\n",
      "-1\n",
      "天安金交所产品逾期理赔无门天安财险履约险成幌子?\n",
      "0.98860764503479\n",
      "-1\n",
      "汇创控股(08202.HK)延迟刊发年度业绩10月2日上午起停牌\n",
      "0.9961422681808472\n",
      "-1\n",
      "5.9亿债务未还，天夏智慧16个银行账户被冻结\n",
      "0.998969316482544\n",
      "-1\n",
      "法国巴黎银行将在比利时裁员800至1000人\n",
      "0.9053449630737305\n",
      "-1\n",
      "龙润茶(02898-HK)中期亏损扩大不派息\n",
      "0.9999605417251587\n",
      "-1\n",
      "康美药业：昔日白马变黑天鹅股债两市集体讨伐\n",
      "0.9940826296806335\n",
      "-1\n",
      "立昂技术股份有限公司关于特定股东股份减持计划提前终止的公告\n",
      "0.9998213648796082\n",
      "-1\n",
      "第一上海(00227.HK)2018年度纯利减少46%至3340.3万港元\n",
      "0.9937437176704407\n",
      "-1\n",
      "平安好医生：投资者群出现匿名恶意诋毁公司不实文章已报案\n",
      "0.9940109848976135\n",
      "-1\n",
      "均安控股(01559-HK)中期少赚15%不派息\n",
      "0.994805634021759\n",
      "-1\n",
      "财联社3月1日讯，绿城中国预计全年净利同比下降50%-60%。\n",
      "0.9961427450180054\n",
      "-1\n",
      "游戏子公司亏损奥飞娱乐称团队解散原团队另起炉灶\n",
      "0.9870989918708801\n",
      "-1\n",
      "怡球资源：融资净偿还677.95万元，融资余额3.8亿元（06-28）\n",
      "0.9129230380058289\n",
      "-1\n",
      "坛金矿业(00621-HK)中期亏损扩大不派息\n",
      "0.9999245405197144\n",
      "-1\n",
      "美商品期货交易委员会对汇丰六机构罚款\n",
      "0.9714890718460083\n",
      "-1\n",
      "人人乐再遇退市危机为转型迟缓付出代价\n",
      "0.9919721484184265\n",
      "-1\n",
      "【美股收盘】谷歌母公司Alphabet大跌拖累纳指下滑标普500再创记录新高\n",
      "0.9989024996757507\n",
      "-1\n",
      "为解决现金储备问题索尼出售奥林巴斯股份\n",
      "0.9540521502494812\n",
      "-1\n",
      "登海种业：融资余额环比下降7.84%，降幅两市第十（06-28）\n",
      "0.9997795224189758\n",
      "-1\n",
      "上海石化02月28日沪股通减持196.37万股\n",
      "0.999944806098938\n",
      "-1\n",
      "上汽集团一季度财报：净利润降15%\n",
      "0.9987252950668335\n",
      "-1\n",
      "帝国集团环球控股(00776-HK)去年亏损扩至3490万元不派息\n",
      "0.9889238476753235\n",
      "-1\n",
      "拉夏贝尔业绩惨淡割韭菜中信证券护驾熊股赚4000万\n",
      "0.988040030002594\n",
      "-1\n",
      "莫高股份股东西藏华富违规减持，被上交所予以监管关注\n",
      "1.0000029802322388\n",
      "-1\n",
      "宝德科技集团(08236)一季度归母净利降25.78%至3595.89万元\n",
      "0.9924636483192444\n",
      "-1\n",
      "原创元力股份频繁买卖资产被疑配合股东减持，实控人4年套现13亿\n",
      "0.9997305274009705\n",
      "-1\n",
      "农业股开盘走弱，登海种业一度触及跌停，敦煌种业、宏辉果蔬、万向德农一度跌逾5%\n",
      "0.9957643747329712\n",
      "-1\n",
      "铭霖控股(01106)年度股东应占亏损扩大1.64倍至2.91亿港元\n",
      "0.9998298287391663\n",
      "-1\n",
      "金诚控股股价跌破1毛，实控人涉非法集资被捕\n",
      "0.9812114238739014\n",
      "-1\n",
      "金诚控股复牌再跌逾40％，市值已不足4亿港元\n",
      "0.9889081120491028\n",
      "-1\n",
      "围海股份陷逾9亿元违规担保及资金占用漩涡\n",
      "0.9992390871047974\n",
      "-1\n",
      "滉达富控股大跌逾36％创五年新低重组要约期失效\n",
      "0.9829859137535095\n",
      "-1\n",
      "汇创控股(08202)延迟刊发2019年度业绩10月2日起停牌\n",
      "0.9753497242927551\n",
      "-1\n",
      "基因测序、区块链、工业大麻……拿什么拯救你，紫鑫药业？\n",
      "0.943121075630188\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "for news_title in news['news_titles']:\n",
    "    result = Sentiment_classification(news_title)\n",
    "    if result['Probility'] >= 0.9 and result['Category'] == -1:\n",
    "        print(result['Sentence'])\n",
    "        print(result['Probility'])\n",
    "        print(result['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
